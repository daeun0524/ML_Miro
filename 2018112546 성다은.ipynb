{
  "cells": [
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "markdown",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport time\nimport copy"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "",
      "attachments": {}
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 그림그리는 함수"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def show_v_table_small(v_table, env):\n    for i in range(env.reward.shape[0]):        \n        print(\"+----------\"*env.reward.shape[1])\n        print(\"|\", end=\"\")\n        for j in range(env.reward.shape[1]):\n            print(\"{0:8.2f}  |\".format(v_table[i,j]),end=\"\")\n        print()\n    print(\"+----------\"*env.reward.shape[1])\n\n# V table 그리기    \ndef show_v_table(v_table, env):    \n    for i in range(env.reward.shape[0]):        \n        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n        print(\"+\")\n        for k in range(3):\n            print(\"|\",end=\"\")\n            for j in range(env.reward.shape[1]):\n                if k==0:\n                    print(\"                 |\",end=\"\")\n                if k==1:\n                        print(\"   {0:8.2f}      |\".format(v_table[i,j]),end=\"\")\n                if k==2:\n                    print(\"                 |\",end=\"\")\n            print()\n    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n    print(\"+\")\n    \n# Q table 그리기\ndef show_q_table(q_table,env):\n    for i in range(env.reward.shape[0]):\n        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n        print(\"+\")\n        for k in range(3):\n            print(\"|\",end=\"\")\n            for j in range(env.reward.shape[1]):\n                if k==0:\n                    print(\"{0:10.2f}       |\".format(q_table[i,j,0]),end=\"\")\n                if k==1:\n                    print(\"{0:6.2f}    {1:6.2f} |\".format(q_table[i,j,3],q_table[i,j,1]),end=\"\")\n                if k==2:\n                    print(\"{0:10.2f}       |\".format(q_table[i,j,2]),end=\"\")\n            print()\n    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n    print(\"+\")\n    \n\n# 정책 policy 화살표로 그리기\ndef show_q_table_arrow(q_table,env):\n    for i in range(env.reward.shape[0]):        \n        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n        print(\"+\")\n        #맨 위에 +-------------+출력\n        for k in range(3):\n            print(\"|\",end=\"\")\n            #다음 줄에 |출력\n            #+-------------+\n            #|\n            for j in range(env.reward.shape[1]):\n                if k==0:\n                    if  env.reward_list1[i][j] ==\"wall\":\n                        print(\"                 |\",end=\"\")\n                    elif np.max(q[i,j,:]) == q[i,j,0]:\n                        print(\"        ↑       |\",end=\"\")\n                    else:\n                        print(\"                 |\",end=\"\")\n                if k==1:\n                    if  env.reward_list1[i][j] ==\"wall\":\n                        print(\"                 |\",end=\"\")\n                    elif np.max(q[i,j,:]) == q[i,j,1] and np.max(q[i,j,:]) == q[i,j,3]:\n                        print(\"      ←  →     |\",end=\"\")\n                    elif np.max(q[i,j,:]) == q[i,j,1]:\n                        print(\"          →     |\",end=\"\")\n                    elif np.max(q[i,j,:]) == q[i,j,3]:\n                        print(\"      ←         |\",end=\"\")\n                    else:\n                        print(\"                 |\",end=\"\")\n                if k==2:\n                    if  env.reward_list1[i][j] ==\"wall\":\n                        print(\"                 |\",end=\"\")\n                    elif np.max(q[i,j,:]) == q[i,j,2]:\n                        print(\"        ↓       |\",end=\"\")\n                    else:\n                        print(\"                 |\",end=\"\")\n            print()\n    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n    print(\"+\")\n    #맨 밑에 +-------------+출력    \n    \n# 정책 policy 화살표로 그리기\ndef show_policy_small(policy,env):\n    for i in range(env.reward.shape[0]):        \n        print(\"+----------\"*env.reward.shape[1],end=\"\")\n        print(\"+\")\n        print(\"|\", end=\"\")\n        for j in range(env.reward.shape[1]):\n            if env.reward_list1[i][j] == \"road\":\n                if policy[i,j] == 0:\n                    print(\"   ↑     |\",end=\"\")\n                elif policy[i,j] == 1:\n                    print(\"   →     |\",end=\"\")\n                elif policy[i,j] == 2:\n                    print(\"   ↓     |\",end=\"\")\n                elif policy[i,j] == 3:\n                    print(\"   ←     |\",end=\"\")\n            elif env.reward_list1[i][j] ==\"wall\":\n                print(\"          |\",end=\"\")\n            else:\n                print(\"          |\",end=\"\")\n        print()\n    print(\"+----------\"*env.reward.shape[1],end=\"\")\n    print(\"+\")\n    \n# 정책 policy 화살표로 그리기\ndef show_policy(policy,env):\n    for i in range(env.reward.shape[0]):        \n        print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n        print(\"+\")\n        for k in range(3):\n            print(\"|\",end=\"\")\n            for j in range(env.reward.shape[1]):\n                if k==0:\n                    print(\"                 |\",end=\"\")\n                if k==1:\n                    if policy[i,j] == 0:\n                        print(\"      ↑         |\",end=\"\")\n                    elif policy[i,j] == 1:\n                        print(\"      →         |\",end=\"\")\n                    elif policy[i,j] == 2:\n                        print(\"      ↓         |\",end=\"\")\n                    elif policy[i,j] == 3:\n                        print(\"      ←         |\",end=\"\")\n                if k==2:\n                    print(\"                 |\",end=\"\")\n            print()\n    print(\"+-----------------\"*env.reward.shape[1],end=\"\")\n    print(\"+\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Agent 구현"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "class Agent():\n    \n    # 1. 행동에 따른 에이전트의 좌표 이동(위, 오른쪽, 아래, 왼쪽) \n    action = np.array([[-1,0],[0,1],[1,0],[0,-1]])\n    \n    # 2. 각 행동별 선택확률\n    select_action_pr = np.array([0.25,0.25,0.25,0.25])\n    \n    # 3. 에이전트의 초기 위치 저장\n    def __init__(self):\n        self.pos = (0,0)\n    \n    # 4. 에이전트의 위치 저장\n    def set_pos(self,position):\n        self.pos = position\n        return self.pos\n    \n    # 5. 에이전트의 위치 불러오기\n    def get_pos(self):\n        return self.pos",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Environment 구현"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "class Environment():\n    \n    # 1. 미로밖(절벽), 길, 목적지와 보상 설정\n    cliff = -3\n    road = -1\n    goal = 1\n    wall=-3\n    \n    # 2. 목적지 좌표 설정\n    goal_position = [5,5]\n    \n    #3. 보상 리스트 숫자\n    reward_list = [[road,wall,road,road,road,road],\n                   [road,road,road,wall,road,wall],\n                   [road,wall,wall,road,road,wall],\n                   [wall,road,road,road,wall,road],\n                   [road,wall,road,road,wall,road],\n                   [road,road,road,road,road,goal]]\n    \n    # 4. 보상 리스트 문자\n    reward_list1 =[[\"road\",\"wall\",\"road\",\"road\",\"road\",\"road\"],\n                   [\"road\",\"road\",\"road\",\"wall\",\"road\",\"wall\"],\n                   [\"road\",\"wall\",\"wall\",\"road\",\"road\",\"wall\"],\n                   [\"wall\",\"road\",\"road\",\"road\",\"wall\",\"road\"],\n                   [\"road\",\"wall\",\"road\",\"road\",\"wall\",\"road\"],\n                   [\"road\",\"road\",\"road\",\"road\",\"road\",\"goal\"]]\n    \n    # 5. 보상 리스트를 array로 설정\n    def __init__(self):\n        self.reward = np.asarray(self.reward_list)    \n\n    # 6. 선택된 에이전트의 행동 결과 반환 (미로밖일 경우 이전 좌표로 다시 복귀)\n    def move(self, agent, action):\n        \n        done = False\n        \n        # 6.1 행동에 따른 좌표 구하기\n        new_pos = agent.pos + agent.action[action]\n        #현재좌표: agent.pos\n        #이동 후 좌표: new_pos\n        \n        # 6.2 현재좌표가 목적지 인지확인\n        if self.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n            reward = self.goal\n            observation = agent.set_pos(agent.pos)\n            done = True\n        # 6.3 이동 후 좌표가 미로 밖인 확인    \n        elif new_pos[0] < 0 or new_pos[0] >= self.reward.shape[0] or new_pos[1] < 0 or new_pos[1] >= self.reward.shape[1]:\n            reward = self.cliff\n            observation = agent.set_pos(agent.pos)\n            done = True\n        # 이동 후 좌표가 벽이라면\n        elif self.reward_list1[new_pos[0]][new_pos[1]] == \"wall\":\n            reward = self.wall\n            observation = agent.set_pos(agent.pos)\n            done = True \n        # 6.4 이동 후 좌표가 길이라면\n        else:\n            observation = agent.set_pos(new_pos)\n            reward = self.reward[observation[0],observation[1]]\n            \n        return observation, reward, done",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 재귀적으로 상태 가치함수를 계산하는 함수"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# 상태 가치 계산\ndef state_value_function(env,agent,G,max_step,now_step):\n    \n    # 1. 감가율 설정\n    gamma = 0.9\n    \n# 2. 현재 위치가 도착지점인지 확인\n    if env.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n        return env.goal\n    \n# 3. 마지막 상태는 보상만 계산\n    if (max_step == now_step):\n        pos1 = agent.get_pos()\n        \n        # 3.1 가능한 모든 행동의 보상을 계산\n        for i in range(len(agent.action)):\n            agent.set_pos(pos1)\n            observation, reward, done = env.move(agent,i)\n            G += agent.select_action_pr[i] * reward\n            \n        return G\n    \n    # 4. 현재 상태의 보상을 계산한 후 다음 step으로 이동\n    else:\n        \n        # 4.1현재 위치 저장\n        pos1 = agent.get_pos()\n        \n        # 4.2 현재 위치에서 가능한 모든 행동을 조사한 후 이동\n        for i in range(len(agent.action)):\n            observation, reward, done = env.move(agent,i)      \n            # 4.2.1 현재 상태에서 보상을 계산\n            G += agent.select_action_pr[i] * reward\n\n            # 4.2.2 이동 후 위치 확인 : 미로밖, 벽, 구멍인 경우 이동전 좌표로 다시 이동\n            if done == True:\n                if observation[0] < 0 or observation[0] >= env.reward.shape[0] or observation[1] < 0 or observation[1] >= env.reward.shape[1]:\n                    agent.set_pos(pos1)\n\n            # 4.2.3 다음 step을 계산\n            next_v = state_value_function(env, agent, 0, max_step, now_step+1)\n            G += agent.select_action_pr[i] * gamma * next_v\n\n            # 4.2.4 현재 위치를 복구\n            agent.set_pos(pos1)\n\n        return G\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 미로의 각 상태의 상태가치함수를 구하는 함수"
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# 1. 환경 초기화\nenv = Environment()\n\n# 2. 에이전트 초기화\nagent = Agent()\n\n# 3. 최대 max_step_number 제한\nmax_step_number = 13\n\n# 4. 계산 시간 저장을 위한 list\ntime_len = []\n\n# 5. 재귀함수 state_value_function을를 이용해 각 상태 가치를 계산\nfor max_step in range(max_step_number):\n    \n    # 5.1 미로 각 상태의 가치를 테이블 형식으로 저장\n    v_table = np.zeros((env.reward.shape[0],env.reward.shape[1]))    \n    start_time = time.time()\n    \n    # 5.2 미로의 각 상태에 대해 state_value_function() 을 이용해 가치를 계산한 후 테이블 형식으로 저장\n    for i in range(env.reward.shape[0]):\n        for j in range(env.reward.shape[1]):\n            agent.set_pos([i,j])\n            v_table[i,j] = state_value_function(env,agent, 0, max_step, 0)\n            \n    # 5.3 max_down에 따른 계산시간 저장\n    time_len.append(time.time()-start_time)\n    print(\"max_step_number = {} total_time = {}(s)\".format(max_step, np.round(time.time()-start_time,2)))\n    \n    show_v_table(np.round(v_table,2),env)\n\n# 6. step 별 계산 시간 그래프 그리기    \nplt.plot(time_len, 'o-k')\nplt.xlabel('max_down')\nplt.ylabel('time(s)')\nplt.legend()\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 재귀적으로 행동가치함수를 계산하는 함수"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# 행동 가치 함수\ndef action_value_function(env, agent, act, G, max_step, now_step):   \n    \n    # 1. 감가율 설정\n    gamma = 0.95\n    \n    # 2. 현재 위치가 목적지인지 확인\n    if env.reward_list1[agent.pos[0]][agent.pos[1]] == \"goal\":\n        return env.goal\n\n    # 3. 마지막 상태는 보상만 계산\n    if (max_step == now_step):\n        observation, reward, done = env.move(agent, act)\n        G += agent.select_action_pr[act]*reward\n        return G\n    \n    # 4. 현재 상태의 보상을 계산한 후 다음 행동과 함께 다음 step으로 이동\n    else:\n        # 4.1현재 위치 저장\n        pos1 = agent.get_pos()\n        observation, reward, done = env.move(agent, act)\n        G += agent.select_action_pr[act] * reward\n        \n        # 4.2 이동 후 위치 확인 : 미로밖, 벽, 구멍인 경우 이동전 좌표로 다시 이동\n        if done == True:\n            if env.reward_list1[observation[0]][observation[1]] ==\"wall\":\n                agent.set_pos(pos1)\n            elif observation[0] < 0 or observation[0] >= env.reward.shape[0] or observation[1] < 0 or observation[1] >= env.reward.shape[1]:\n                agent.set_pos(pos1)\n            \n        # 4.3 현재 위치를 다시 저장\n        pos1 = agent.get_pos()\n        \n        # 4.4 현재 위치에서 가능한 모든 행동을 선택한 후 이동\n        for i in range(len(agent.action)):\n            agent.set_pos(pos1)\n            next_v = action_value_function(env, agent, i, 0, max_step, now_step+1)\n            G += agent.select_action_pr[i] * gamma * next_v\n        return G",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 미로의 각 상태의 행동가치함수를 구하는 함수"
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# 재귀적으로 행동의 가치를 계산\n\n# 1. 환경 초기화\nenv = Environment()\n\n# 2. 에이전트 초기화\nagent = Agent()\nnp.random.seed(0)\n\n# 3. 현재부터 max_step 까지 계산\nmax_step_number =10\n\n# 4. 모든 상태에 대해\nfor max_step in range(9,max_step_number):\n    # 4.1 미로 상의 모든 상태에서 가능한 행동의 가치를 저장할 테이블을 정의\n    print(\"max_step = {}\".format(max_step))\n    q_table = np.zeros((env.reward.shape[0], env.reward.shape[1],len(agent.action)))\n    for i in range(env.reward.shape[0]):\n        for j in range(env.reward.shape[1]):\n            # 4.2 모든 행동에 대해\n            for action in range(len(agent.action)):\n                # 4.2.1 에이전트의 위치를 초기화\n                agent.set_pos([i,j])\n                # 4.2.2 현재 위치에서 행동 가치를 계산\n                q_table[i ,j,action] = action_value_function(env, agent, action, 0, max_step, 0)\n\n    q = np.round(q_table,2)\n    print(\"Q - table\")\n    show_q_table(q, env)\n    print(\"High actions Arrow\")\n    show_q_table_arrow(q,env)\n    print()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 반복 정책 평가"
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "![image.png](attachment:image.png)",
      "attachments": {
        "image.png": {
          "image/png": "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"
        }
      }
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# 반복 정책 평가\nnp.random.seed(0)\nenv = Environment()\nagent = Agent()\ngamma = 0.9\n\n# 1. 모든 𝑠∈𝑆^에 대해서 배열 𝑉(𝑠)=0으로 초기화\nv_table = np.zeros((env.reward.shape[0],env.reward.shape[1]))\n\nprint(\"start Iterative Policy Evaluation\")\n\nk = 1\nprint()\nprint(\"V0(S)   k = 0\")\n\n# 초기화된 V 테이블 출력\nshow_v_table(np.round(v_table,2),env)\n\n# 시작 시간 변수에 저장\nstart_time = time.time()\n\n# 반복\nwhile(True):    \n    # 2. Δ←0\n    delta = 0\n    # 3. v←(𝑠)\n    # 계산전 가치를 저장\n    temp_v = copy.deepcopy(v_table)\n    # 4. 모든 𝑠∈𝑆에 대해 : \n    for i in range(env.reward.shape[0]):\n        for j in range(env.reward.shape[1]):\n            G = 0\n            # 5. 가능한 모든 행동으로 다음상태만 이용해 𝑉(𝑠) 계산\n            for action in range(len(agent.action)):\n                agent.set_pos([i,j])\n                observation, reward, done = env.move(agent, action)\n                \n#                 print(\"s({0}): {1:5s} : {2:0.2f} = {3:0.2f} *({4:0.2f} +  {5:0.2f} *  {6:0.2f})\".format(i*env.reward.shape[0]+j,dic[action],agent.select_action_pr[action] * (reward + gamma*V[observation[0],observation[1]]), agent.select_action_pr[action],reward,gamma,V[observation[0],observation[1]]))\n\n                G += agent.select_action_pr[action] * (reward + gamma*v_table[observation[0],observation[1]])                    \n\n#             print(\"V{2}({0}) :sum = {1:.2f}\".format(i*env.reward.shape[0]+j,total,k))\n#             print()\n            v_table[i,j] = G\n    # 6. ∆←max⁡(∆,|v−𝑉(𝑠)|)\n    # 계산전과 계산후의 가치 차이 계산\n    delta = np.max([delta, np.max(np.abs(temp_v-v_table))])\n    \n    end_time = time.time()        \n    print(\"V{0}(S) : k = {1:3d}    delta = {2:0.6f} total_time = {3}\".format(k,k, delta,np.round(end_time-start_time),2))\n    show_v_table(np.round(v_table,2),env)                \n    k +=1\n\n    # 7. ∆ <𝜃가 작은 양수 일 때까지 반복\n\n    if delta < 0.000001:\n        break\n        \nend_time = time.time()        \nprint(\"total_time = {}\".format(np.round(end_time-start_time),2))\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 정책 반복"
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "def policy_evalution(env, agent, v_table, policy):\n    gamma = 0.9\n    while(True):\n        # Δ←0\n        delta = 0\n        #  v←𝑉(𝑠)\n        temp_v = copy.deepcopy(v_table)\n        # 모든 𝑠∈𝑆에 대해 :\n        for i in range(env.reward.shape[0]):\n            for j in range(env.reward.shape[1]):\n                # 에이전트를 지정된 좌표에 위치시킨후 가치함수를 계산\n                agent.set_pos([i,j])\n                # 현재 정책의 행동을 선택\n                action = policy[i,j]\n                observation, reward, done = env.move(agent, action)\n                v_table[i,j] = reward + gamma * v_table[observation[0],observation[1]]\n        # ∆←max⁡(∆,|v−𝑉(𝑠)|)\n        # 계산전과 계산후의 가치의 차이를 계산\n        delta = np.max([delta, np.max(np.abs(temp_v-v_table))])  \n                \n        # 7. ∆ <𝜃가 작은 양수 일 때까지 반복\n        if delta < 0.000001:\n            break\n    return v_table, delta\n\n\ndef policy_improvement(env, agent, v_table, policy):\n    \n    # 67페이지 아래 누락 되어있습니다\n    gamma = 0.9  \n    \n    # policyStable ← true \n    policyStable = True\n\n    # 모든 s∈S에 대해：\n    for i in range(env.reward.shape[0]):\n        for j in range(env.reward.shape[1]):            \n            # 𝑜𝑙𝑑−𝑎𝑐𝑡𝑖𝑜𝑛←π(s) \n            old_action = policy[i,j]            \n            # 가능한 행동중 최댓값을 가지는 행동을 선택\n            temp_action = 0\n            temp_value =  -1e+10           \n            for action in range(len(agent.action)):\n                agent.set_pos([i,j])\n                observation, reward, done = env.move(agent,action)\n                if temp_value < reward + gamma * v_table[observation[0],observation[1]]:\n                    temp_action = action\n                    temp_value = reward + gamma * v_table[observation[0],observation[1]]\n            # 만약 𝑜𝑙𝑑−𝑎𝑐𝑡𝑖𝑜𝑛\"≠π(s)\"라면， \"policyStable ← False\" \n            # old-action과 새로운 action이 다른지 체크\n            if old_action != temp_action :\n                policyStable = False\n            policy[i,j] = temp_action\n    return policy, policyStable\n\n# 정책 반복\n# 환경과 에이전트에 대한 초기 설정\nnp.random.seed(0)\nenv = Environment()\nagent = Agent()\n\n# 1. 초기화\n# 모든 𝑠∈𝑆에 대해 𝑉(𝑠)∈𝑅과 π(𝑠)∈𝐴(𝑠)를 임의로 설정\nv_table =  np.random.rand(env.reward.shape[0], env.reward.shape[1])\npolicy = np.random.randint(0, 4,(env.reward.shape[0], env.reward.shape[1]))\n\nprint(\"Initial random V(S)\")\nshow_v_table(np.round(v_table,2),env)\nprint()\nprint(\"Initial random Policy π0(S)\")\nshow_policy(policy,env)\nprint(\"start policy iteration\")\n\n# 시작 시간을 변수에 저장\nstart_time = time.time()\n\nmax_iter_number = 20000\nfor iter_number in range(max_iter_number):\n    \n    # 2.정책평가\n    v_table, delta = policy_evalution(env, agent, v_table, policy)\n\n    # 정책 평가 후 결과 표시                                            \n    print(\"\")\n    print(\"Vπ{0:}(S) delta = {1:.10f}\".format(iter_number,delta))\n    show_v_table(np.round(v_table,2),env)\n    print()    \n    \n    \n    # 3.정책개선\n    policy, policyStable = policy_improvement(env, agent, v_table, policy)\n\n    # policy 변화 저장\n    print(\"policy π{}(S)\".format(iter_number+1))\n    show_policy(policy,env)\n    # 하나라도 old-action과 새로운 action이 다르다면 '2. 정책평가'를 반복\n    if(policyStable == True):\n        break\n\n        \nprint(\"total_time = {}\".format(time.time()-start_time))\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 가치반복"
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": true
      },
      "cell_type": "code",
      "source": "def finding_optimal_value_function(env, agent, v_table):\n    k = 1\n    gamma = 0.9\n    while(True):\n        # Δ←0\n        delta=0\n        #  v←𝑉(𝑠)\n        temp_v = copy.deepcopy(v_table)\n\n        # 모든 𝑠∈𝑆에 대해 :\n        for i in range(env.reward.shape[0]):\n            for j in range(env.reward.shape[1]):\n                temp = -1e+10\n#                 print(\"s({0}):\".format(i*env.reward.shape[0]+j))\n                # 𝑉(𝑠)← max(a)⁡∑𝑃(𝑠'|𝑠,𝑎)[𝑟(𝑠,𝑎,𝑠') +𝛾𝑉(𝑠')]\n                # 가능한 행동을 선택\n                for action in range(len(agent.action)):\n                    agent.set_pos([i,j])\n                    observation, reward, done = env.move(agent, action)\n#                     print(\"{0:.2f} = {1:.2f} + {2:.2f} * {3:.2f}\" .format(reward + gamma* v_table[observation[0],observation[1]],reward, gamma,v_table[observation[0],observation[1]]))\n                    #이동한 상태의 가치가 temp보다 크면\n                    if temp < reward + gamma*v_table[observation[0],observation[1]]:\n                        # temp 에 새로운 가치를 저장\n                        temp = reward + gamma*v_table[observation[0],observation[1]]  \n#                 print(\"V({0}) :max = {1:.2f}\".format(i*env.reward.shape[0]+j,temp))\n#                 print()\n                # 이동 가능한 상태 중 가장 큰 가치를 저장\n                v_table[i,j] = temp\n\n        #  ∆←max⁡(∆,|v−𝑉(𝑠)|)\n        # 이전 가치와 비교해서 큰 값을 delta에 저장\n        # 계산전과 계산후의 가치의 차이 계산\n        delta = np.max([delta, np.max(np.abs(temp_v-v_table))])  \n        # 7. ∆ <𝜃가 작은 양수 일 때까지 반복\n        if delta < 0.0000001:\n            break\n            \n#         if k < 4 or k > 150:\n#             print(\"V{0}(S) : k = {1:3d}    delta = {2:0.6f}\".format(k,k, delta))\n#             show_v_table(np.round(v_table,2),env)\n        print(\"V{0}(S) : k = {1:3d}    delta = {2:0.6f}\".format(k,k, delta))\n        show_v_table(np.round(v_table,2),env)\n        k +=1\n        \n    return v_table\n\ndef policy_extraction(env, agent, v_table, optimal_policy):\n\n    gamma = 0.9\n    \n    #정책 𝜋를 다음과 같이 추출\n    # 𝜋(𝑠)← argmax(a)⁡∑𝑃(𝑠'|𝑠,𝑎)[𝑟(𝑠,𝑎,𝑠') +𝛾𝑉(𝑠')]\n    # 모든 𝑠∈𝑆에 대해 : \n    for i in range(env.reward.shape[0]):\n        for j in range(env.reward.shape[1]):\n            temp =  -1e+10\n            # 가능한 행동중 가치가 가장높은 값을 policy[i,j]에 저장\n            for action in range(len(agent.action)):\n                agent.set_pos([i,j])\n                observation, reward, done = env.move(agent,action)\n                if temp < reward + gamma * v_table[observation[0],observation[1]]:\n                    optimal_policy[i,j] = action\n                    temp = reward + gamma * v_table[observation[0],observation[1]]\n                \n    return optimal_policy\n\n\n# 가치 반복\n\n# 환경, 에이전트를 초기화\nnp.random.seed(0)\nenv = Environment()\nagent = Agent()\n\n# 초기화\n# 모든 𝑠∈𝑆^+에 대해 𝑉(𝑠)∈𝑅을 임의로 설정\nv_table =  np.random.rand(env.reward.shape[0], env.reward.shape[1])\n\nprint(\"Initial random V0(S)\")\nshow_v_table(np.round(v_table,2),env)\nprint()\n\noptimal_policy = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n\nprint(\"start Value iteration\")\nprint()\n\n# 시작 시간 변수에 저장\nstart_time = time.time()\n\nv_table = finding_optimal_value_function(env, agent, v_table)\n\noptimal_policy = policy_extraction(env, agent, v_table, optimal_policy)\n\n                \nprint(\"total_time = {}\".format(np.round(time.time()-start_time),2))\nprint()\nprint(\"Optimal policy\")\nshow_policy(optimal_policy, env)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 에피소드 생성"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def generate_episode(env, agent, first_visit):\n    gamma = 0.09\n    # 에피소드를 저장할 리스트\n    episode = []\n    # 이전에 방문여부 체크\n    visit = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n    \n    # 에이전트가 모든 상태에서 출발할 수 있게 출발지점을 무작위로 설정\n    i = np.random.randint(0,env.reward.shape[0])\n    j = np.random.randint(0,env.reward.shape[1])\n    agent.set_pos([i,j])    \n    #에피소드의 수익을 초기화\n    G = 0\n    #감쇄율의 지수\n    step = 0\n    max_step = 100\n    # 에피소드 생성\n    for k in range(max_step):\n        pos = agent.get_pos()            \n        action = np.random.randint(0,len(agent.action))            \n        observaetion, reward, done = env.move(agent, action)    \n        \n        if first_visit:\n            # 에피소드에 첫 방문한 상태인지 검사 :\n            # visit[pos[0],pos[1]] == 0 : 첫 방문\n            # visit[pos[0],pos[1]] == 1 : 중복 방문\n            if visit[pos[0],pos[1]] == 0:   \n                # 에피소드가 끝날때까지 G를 계산\n                G += gamma**(step) * reward        \n                # 방문 이력 표시\n                visit[pos[0],pos[1]] = 1\n                step += 1               \n                # 방문 이력 저장(상태, 행동, 보상)\n                episode.append((pos,action, reward))\n        else:\n            G += gamma**(step) * reward\n            step += 1                   \n            episode.append((pos,action,reward))            \n\n        # 에피소드가 종료했다면 루프에서 탈출\n        if done == True:                \n            break        \n            \n    return i, j, G, episode",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### First-visit and Every-Visit MC Prediction"
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# first-visit MC and every-visit MC prediction\nnp.random.seed(0)\n# 환경, 에이전트를 초기화\nenv = Environment()\nagent = Agent()\n\n# 임의의 상태 가치 함수𝑉\nv_table = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n\n# 상태별로 에피소드 출발횟수를 저장하는 테이블\nv_start = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n\n# 상태별로 도착지점 도착횟수를 저장하는 테이블\nv_success = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n\n# 𝑅𝑒𝑡𝑢𝑟𝑛(𝑠)←빈 리스트 (모든 s∈𝑆에 대해)\nReturn_s = [[[] for j in range(env.reward.shape[1])] for i in range(env.reward.shape[0])]\n\n# 최대 에피소드 수를 지정\nmax_episode = 100000\n\n# first visit 를 사용할지 every visit를 사용할 지 결정\n# first_visit = True : first visit\n# first_visit = False : every visit\nfirst_visit = True\nif first_visit:\n    print(\"start first visit MC\")\nelse : \n    print(\"start every visit MC\")\nprint()\n\nfor epi in tqdm(range(max_episode)):\n    \n    i,j,G,episode = generate_episode(env, agent, first_visit)\n    \n    # 수익 𝐺를 𝑅𝑒𝑡𝑢𝑟𝑛(𝑠)에 추가(append)\n    Return_s[i][j].append(G)\n    \n    # 에피소드 발생 횟수 계산\n    episode_count = len(Return_s[i][j])\n    # 상태별 발생한 수익의 총합 계산\n    total_G = np.sum(Return_s[i][j])\n    # 상태별 발생한 수익의 평균 계산\n    v_table[i,j] = total_G / episode_count\n    \n  # 도착지점에 도착(reward = 1)했는지 체크    \n    # episode[-1][2] : 에피소드 마지막 상태의 보상\n    if episode[-1][2] == 1:\n        v_success[i,j] += 1\n\n# 에피소드 출발 횟수 저장 \nfor i in range(env.reward.shape[0]):\n    for j in range(env.reward.shape[1]):\n        v_start[i,j] = len(Return_s[i][j])\n        \nprint(\"V(s)\")\nshow_v_table(np.round(v_table,2),env)\nprint(\"V_start_count(s)\")\nshow_v_table(np.round(v_start,2),env)\nprint(\"V_success_pr(s)\")\nshow_v_table(np.round(v_success/v_start,2),env)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Incremental mean 을 이용하는 몬테카를로 Prediction 알고리즘"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#Incremental mean 을 이용하는 몬테카를로 Prediction 알고리즘\nnp.random.seed(0)\n# 환경, 에이전트를 초기화\nenv = Environment()\nagent = Agent()\n\n# 임의의 상태 가치 함수𝑉\nv_table = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n\n# 추가\n# 상태를 방문한 횟수를 저장하는 테이블\nv_visit = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n\n# 삭제\n# # 𝑅𝑒𝑡𝑢𝑟𝑛(𝑠)←빈 리스트 (모든 s∈𝑆에 대해) : \n# Return_s = [[[] for j in range(env.reward.shape[1])] for i in range(env.reward.shape[0])]\n\n# 최대 에피소드 수와 에피소드 최대 길이지정\nmax_episode = 100000\n\n# first visit을 사용할지 every visit을 사용할 지 결정\n# first_visit = True : first visit\n# first_visit = False : every visit\nfirst_visit = True\nif first_visit:\n    print(\"start first visit MC\")\nelse : \n    print(\"start every visit MC\")\nprint()\n\nfor epi in tqdm(range(max_episode)):\n    \n    i,j,G,episode = generate_episode(env, agent, first_visit)\n    \n    # 삭제\n    # 수익 𝐺를 𝑅𝑒𝑡𝑢𝑟𝑛(𝑠)에 추가(append)\n    #Return_s[i][j].append(G)\n    \n    # 에피소드 발생 횟수 계산\n    #episode_count = len(Return_s[i][j])\n    # 상태별 발생한 수익의 총합 계산\n    #total_G = np.sum(Return_s[i][j])\n    # 상태별 발생한 수익의 평균 계산\n    #v_table[i,j] = total_G / episode_count\n    \n    #Return_length[i][j].append(len(episode))\n\n    #if episode[-1][2] == 1:\n    #    v_success[i,j] += 1    \n    \n    # Incremental mean  평균을 계산\n    v_visit[i,j] += 1\n    v_table[i,j] += 1 / v_visit[i,j] * (G - v_table[i,j])\n    \n      \nprint(\"V(s)\")\nshow_v_table(np.round(v_table,2),env)\nprint()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "###  에피소드 분리를 이용하는 Firsit-visit 몬테카를로 방법과 Every-visit 몬테카를로 방법의 Prediction 알고리즘"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "def generate_episode(env, agent, first_visit):\n    gamma = 0.09\n    # 에피소드를 저장할 리스트\n    episode = []\n    # 이전에 방문여부 체크\n    visit = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n    \n    # 에이전트는 항상 (0,0)에서 출발\n    i = 0\n    j = 0\n    agent.set_pos([i,j])    \n    #에피소드의 수익을 초기화\n    G = 0\n    #감쇄율의 지수\n    step = 0\n    max_step = 100\n    # 에피소드 생성\n    for k in range(max_step):\n        pos = agent.get_pos()            \n        action = np.random.randint(0,len(agent.action))            \n        observaetion, reward, done = env.move(agent, action)    \n        \n        if first_visit:\n            # 에피소드에 첫 방문한 상태인지 검사 :\n            # visit[pos[0],pos[1]] == 0 : 첫 방문\n            # visit[pos[0],pos[1]] == 1 : 중복 방문\n            if visit[pos[0],pos[1]] == 0:   \n                # 에피소드가 끝날때까지 G를 계산\n                G += gamma**(step) * reward        \n                # 방문 이력 표시\n                visit[pos[0],pos[1]] = 1\n                step += 1               \n                # 방문 이력 저장(상태, 행동, 보상)\n                episode.append((pos,action, reward))\n        else:\n            G += gamma**(step) * reward\n            step += 1                   \n            episode.append((pos,action, reward))            \n\n        # 에피소드가 종료했다면 루프에서 탈출\n        if done == True:                \n            break        \n            \n    return i, j, G, episode\n\n#에피소드 분리를 이용하는 몬테카를로 Prediction 알고리즘\nnp.random.seed(0)\n# 환경, 에이전트를 초기화\nenv = Environment()\nagent = Agent()\n\n# 임의의 상태 가치 함수𝑉\nv_table = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n\n# 상태를 방문한 횟수를 저장하는 테이블\nv_visit = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n\n# 최대 에피소드 수와 에피소드 최대 길이를 지정\nmax_episode = 99999\n\n# first visit 를 사용할지 every visit를 사용할 지 결정\n# first_visit = True : first visit\n# first_visit = False : every visit\nfirst_visit = True\nif first_visit:\n    print(\"start first visit MC\")\nelse : \n    print(\"start every visit MC\")\nprint()\n\ngamma = 0.09\nfor epi in tqdm(range(max_episode)):\n    \n    i,j,G,episode = generate_episode(env, agent, first_visit)\n    for step_num in range(len(episode)):\n        G = 0\n        # episode[step_num][0][0] : step_num번째 방문한 상태의 x 좌표\n        # episode[step_num][0][1] : step_num번째 방문한 상태의 y 좌표\n        # episode[step_num][1] : step_num번째 상태에서 선택한 행동\n        i = episode[step_num][0][0]\n        j = episode[step_num][0][1]\n        # 에피소드 시작점을 카운트\n        v_visit[i,j] += 1\n\n        # 서브 에피소드 (episode[step_num:])의 출발부터 끝까지 보수 G를 계산\n        # k[2] : episode[step_num][2] 과 같으며 step_num 번째 받은 보상\n        # step : 감쇄율\n        for step, k in enumerate(episode[step_num:]):\n            G += gamma**(step)*k[2]\n            \n        # Incremental mean  평균을 계산\n        v_table[i,j] += 1 / v_visit[i,j] * (G - v_table[i,j])\n           \nprint(\"V(s)\")\nshow_v_table(np.round(v_table,2),env)\nprint(\"V_start_count(s)\")\nshow_v_table(np.round(v_visit,2),env)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### e-greedy 확률 변화"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "policy = np.array((0.1,0.2,0.3,0.4))\nup_list=[]\nright_list=[]\ndown_list=[]\nleft_list=[]\nprint(\"episilon |   a1  |    a2   |    a3  |   a4  \")\nfor epsilon in reversed(range(0, 11)):\n    epsilon /= 10\n    up = epsilon / len(policy)\n    right = epsilon / len(policy)\n    down = epsilon / len(policy)\n    left = 1 -  epsilon + epsilon / len(policy)\n    print(\"{0:}      | {1:.3f} |  {2:.3f}  |  {3:.3f} |  {4:.3f}\".format(np.round(epsilon,3),np.round(up,3),np.round(right,3),np.round(down,3),np.round(left,3)))\n#     policy_list.append((up,right,down,left))\n#     up_list.append(up)\n#     right_list.append(right)\n#     down_list.append(down)\n#     left_list.append(left)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### ϵ-정책을 이용하는 몬테카를로 control 알고리즘"
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "def generate_episode_with_policy(env, agent, first_visit, policy):\n    gamma = 0.09\n    # 에피소드를 저장할 리스트\n    episode = []\n    # 이전에 방문여부 체크\n    visit = np.zeros((env.reward.shape[0], env.reward.shape[1],len(agent.action)))\n    \n    # 에이전트는 항상 (0,0)에서 출발\n    i = 0\n    j = 0\n    agent.set_pos([i,j])    \n    #에피소드의 수익을 초기화\n    G = 0\n    #감쇄율의 지수\n    step = 0\n    max_step = 100\n    # 에피소드 생성\n    for k in range(max_step):\n        pos = agent.get_pos()        \n        # 현재 상태의 정책을 이용해 행동을 선택한 후 이동\n        action = np.random.choice(range(0,len(agent.action)), p=policy[pos[0],pos[1],:]) \n        observaetion, reward, done = env.move(agent, action)    \n        \n        if first_visit:\n            # 에피소드에 첫 방문한 상태인지 검사 :\n            # visit[pos[0],pos[1]] == 0 : 첫 방문\n            # visit[pos[0],pos[1]] == 1 : 중복 방문\n            if visit[pos[0],pos[1],action] == 0:   \n                # 에피소드가 끝날때까지 G를 계산\n                G += gamma**(step) * reward        \n                # 방문 이력 표시\n                visit[pos[0],pos[1],action] = 1\n                step += 1               \n                # 방문 이력 저장(상태, 행동, 보상)\n                episode.append((pos,action, reward))\n        else:\n            G += gamma**(step) * reward\n            step += 1                   \n            episode.append((pos,action,reward))            \n\n        # 에피소드가 종료했다면 루프에서 탈출\n        if done == True:                \n            break        \n            \n    return i, j, G, episode\n\n# ϵ-정책을 이용하는 몬테카를로 control 알고리즘\nnp.random.seed(0)\n# 환경, 에이전트를 초기화\nenv = Environment()\nagent = Agent()\n\n# 모든 𝑠∈𝑆,𝑎∈𝐴(𝑆)에 대해 초기화:\n# # 𝑄(𝑠,𝑎)←임의의 값 (행동 개수, 미로 세로, 미로 가로)\nQ_table = np.random.rand(env.reward.shape[0], env.reward.shape[1],len(agent.action))\nprint(\"Initial Q(s,a)\")\nshow_q_table(Q_table,env)\n\n# 상태를 방문한 횟수를 저장하는 테이블\nQ_visit = np.zeros((env.reward.shape[0], env.reward.shape[1],len(agent.action)))\n\n# 미로 모든 상태에서 최적 행동을 저장하는 테이블\n# 각 상태에서 Q 값이 가장 큰 행동을 선택 후 optimal_a 에 저장\noptimal_a = np.zeros((env.reward.shape[0],env.reward.shape[1]))\nfor i in range(env.reward.shape[0]):\n    for j in range(env.reward.shape[1]):\n        optimal_a[i,j] = np.argmax(Q_table[i,j,:])\nprint(\"initial optimal_a\")\nshow_policy(optimal_a,env)\n\n# π(𝑠,𝑎)←임의의 𝜖−탐욕 정책\n# 무작위로 행동을 선택하도록 지정\npolicy = np.zeros((env.reward.shape[0], env.reward.shape[1],len(agent.action)))\n\n# 한 상태에서 가능한 확률의 합이 1이 되도록 계산\nepsilon = 0.8\nfor i in range(env.reward.shape[0]):\n    for j in range(env.reward.shape[1]):\n        for k in range(len(agent.action)):\n            if optimal_a[i,j] == k:\n                policy[i,j,k] = 1 - epsilon + epsilon/len(agent.action)\n            else:\n                policy[i,j,k] = epsilon/len(agent.action)\nprint(\"Initial Policy\")\nshow_q_table(policy,env)\n\n\n# 최대 에피소드 수 길이를 지정\nmax_episode = 10000\n\n# first visit 를 사용할지 every visit를 사용할 지 결정\n# first_visit = True : first visit\n# first_visit = False : every visit\nfirst_visit = True\nif first_visit:\n    print(\"start first visit MC\")\nelse : \n    print(\"start every visit MC\")\nprint()\n\ngamma = 0.09\nfor epi in tqdm(range(max_episode)):\n# for epi in range(max_episode):\n\n    # π를 이용해서 에피소드 1개를 생성\n    x,y,G,episode = generate_episode_with_policy(env, agent, first_visit, policy)\n    \n    for step_num in range(len(episode)):\n        G = 0\n        # episode[step_num][0][0] : step_num번째 방문한 상태의 x 좌표\n        # episode[step_num][0][1] : step_num번째 방문한 상태의 y 좌표\n        # episode[step_num][1] : step_num번째 상태에서 선택한 행동\n        i = episode[step_num][0][0]\n        j = episode[step_num][0][1]\n        action = episode[step_num][1]\n        \n        # 에피소드 시작점을 카운트\n        Q_visit[i,j,action] += 1\n\n        # 서브 에피소드 (episode[step_num:])의 출발부터 끝까지 수익 G를 계산\n        # k[2] : episode[step_num][2] 과 같으며 step_num 번째 받은 보상\n        # step : 감쇄율\n        for step, k in enumerate(episode[step_num:]):\n            G += gamma**(step)*k[2]\n\n        # Incremental mean : 𝑄(𝑠,𝑎)←𝑎𝑣𝑒𝑟𝑎𝑔𝑒(𝑅𝑒𝑡𝑢𝑟𝑛(𝑠,𝑎)) \n        Q_table[i,j,action] += 1 / Q_visit[i,j,action]*(G-Q_table[i,j,action])\n    \n    # (c) 에피소드 안의 각 s에 대해서 :\n    # 미로 모든 상태에서 최적 행동을 저장할 공간 마련\n    # 𝑎∗ ←argmax_a 𝑄(𝑠,𝑎)\n    for i in range(env.reward.shape[0]):\n        for j in range(env.reward.shape[1]):\n            optimal_a[i,j] = np.argmax(Q_table[i,j,:])            \n   \n    # 모든 𝑎∈𝐴(𝑆) 에 대해서 :\n    # 새로 계산된 optimal_a 를 이용해서 행동 선택 확률 policy (π) 갱신\n    epsilon = 1 - epi/max_episode\n\n    for i in range(env.reward.shape[0]):\n        for j in range(env.reward.shape[1]):\n            for k in range(len(agent.action)):\n                if optimal_a[i,j] == k:\n                    policy[i,j,k] = 1 - epsilon + epsilon/len(agent.action)\n                else:\n                    policy[i,j,k] = epsilon/len(agent.action)\n\nprint(\"Final Q(s,a)\")\nshow_q_table(Q_table,env)\nprint(\"Final policy\")\nshow_q_table(policy,env)\nprint(\"Final optimal_a\")\nshow_policy(optimal_a,env)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### TD(0) prediction"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# TD(0) prediction\nnp.random.seed(0)\n# 환경, 에이전트를 초기화\nenv = Environment()\nagent = Agent()\ngamma = 0.9\n\n#초기화 : \n#π← 평가할 정책\n# 가능한 모든 행동이 무작위로 선택되도록 지정\n#𝑉← 임의의 상태가치 함수\nV = np.zeros((env.reward.shape[0], env.reward.shape[1]))\n\n# 최대 에피소드, 에피소드의 최대 길이를 지정\nmax_episode = 10000\nmax_step = 100\n\nalpha = 0.01\n\nprint(\"start TD(0) prediction\")\n\n# 각 에피소드에 대해 반복 :\nfor epi in tqdm(range(max_episode)):\n    delta =0\n    # s 를 초기화\n    i = 0\n    j = 0\n    agent.set_pos([i,j])\n\n    #  에피소드의 각 스텝에 대해 반복 :\n    for k in range(max_step):\n        pos = agent.get_pos()\n        # a←상태 𝑠 에서 정책 π에 의해 결정된 행동 \n        # 가능한 모든 행동이 무작위로 선택되게 함\n        action = np.random.randint(0,4)\n        # 행동 a 를 취한 후 보수 r과 다음 상태 s’를 관측\n        # s←𝑠'\n        observation, reward, done = env.move(agent,action)\n        # V(𝑠)←V(𝑠)+ α[𝑟+𝛾𝑉(𝑠^)−𝑉(𝑠)]\n        V[pos[0],pos[1]] += alpha * (reward + gamma * V[observation[0],observation[1]] - V[pos[0],pos[1]])\n        # s가 마지막 상태라면 종료\n        if done == True:\n            break\n            \nprint(\"V(s)\")\nshow_v_table(np.round(V,2),env)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### 𝜖−𝑔𝑟𝑒𝑒𝑑𝑦, 𝑔𝑟𝑒𝑒𝑑𝑦  함수 작성"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#  𝜖−𝑔𝑟𝑒𝑒𝑑𝑦, 𝑔𝑟𝑒𝑒𝑑𝑦  함수 작성\ndef e_greedy(Q_table,agent,epsilon):\n    pos = agent.get_pos()\n    greedy_action = np.argmax(Q_table[pos[0],pos[1],:])\n    pr = np.zeros(4)\n    for i in range(len(agent.action)):\n        if i == greedy_action:\n            pr[i] = 1 - epsilon + epsilon/len(agent.action)\n        else:\n            pr[i] = epsilon / len(agent.action)\n\n    return np.random.choice(range(0,len(agent.action)), p=pr)    \n\ndef greedy(Q_table,agent,epsilon):\n    pos = agent.get_pos()\n    return np.argmax(Q_table[pos[0],pos[1],:])   ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### TD(0) coltrol : SARSA"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "# TD(0) control : SARSA\nnp.random.seed(0)\n\n# 환경, 에이전트를 초기화\nenv = Environment()\nagent = Agent()\ngamma = 0.9\n\n# 모든 𝑠∈𝑆,𝑎∈𝐴(𝑆)에 대해 초기화:\n# 𝑄(𝑠,𝑎)←임의의 값\nQ_table = np.random.rand(env.reward.shape[0], env.reward.shape[1],len(agent.action))\n\n# Q(𝑡𝑒𝑟𝑚𝑖𝑛𝑎𝑙−𝑠𝑡𝑎𝑡𝑒,𝑎)=0\nQ_table[2,2,:] = 0\n\nmax_episode = 10000\nmax_step = 100\n\nprint(\"start TD(0) control : SARSA\")\nalpha = 0.1\nepsilon = 0.8\n\n# 각 에피소드에 대해서 반복 :\nfor epi in tqdm(range(max_episode)):\n    dleta = 0\n    # S 를 초기화\n    i = 0\n    j = 0\n    agent.set_pos([i,j])\n    temp =0\n    \n    # s 에서 행동 정책(Behavior policy)으로 행동 a를 선택 ( 예 : ε-greedy\n    action = e_greedy(Q_table,agent,epsilon)\n\n    # 에피소드의 각 스텝에 대해서 반복 :\n    for k in range(max_step):\n        pos = agent.get_pos()\n#         Q_visit_count[action, pos[0],pos[1]] +=1\n        # 행동 a 를 취한 후 보상 r과 다음 상태 s^'  관측\n        observation, reward, done = env.move(agent, action)\n\n        # s^' 에서 타깃 정책(Target policy)으로 행동 a^'를 선택 ( 예 : ε-greedy\n        next_action = e_greedy(Q_table,agent,epsilon)\n        \n        # Q(𝑆,𝐴)←Q(𝑆,𝐴) + α[𝑅+𝛾𝑄(𝑆',𝐴')−𝑄(𝑆,𝐴)]\n        Q_table[pos[0],pos[1],action] += alpha * (reward + gamma * Q_table[observation[0], observation[1],next_action] - Q_table[pos[0],pos[1],action])\n\n        # s←s^' ; a←a^'\n        action = next_action\n        \n        # s가 마지막 상태라면 종료\n        if done == True:\n            break\n        \n# 학습된 정책에서 최적 행동 추출\noptimal_policy = np.zeros((env.reward.shape[0], env.reward.shape[1]))\nfor i in range(env.reward.shape[0]):\n    for j in range(env.reward.shape[1]):\n        optimal_policy[i,j] = np.argmax(Q_table[i,j,:])\n\nprint(\"SARSA : Q(s,a)\")\nshow_q_table(np.round(Q_table,2),env)\nprint(\"SARSA :optimal policy\")\nshow_policy(optimal_policy,env)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### TD(0) coltrol : Q-learning"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#  TD(0) contro : Q-learning\n\n# 환경, 에이전트를 초기화\nenv = Environment()\nagent = Agent()\ngamma = 0.9\nnp.random.seed(0)\n\n# 모든 𝑠∈𝑆,𝑎∈𝐴(𝑆)에 대해 초기화:\n# 𝑄(𝑠,𝑎)←임의의 값\nQ_table = np.random.rand(env.reward.shape[0], env.reward.shape[1],len(agent.action))\n\n# Q(𝑡𝑒𝑟𝑚𝑖𝑛𝑎𝑙−𝑠𝑡𝑎𝑡𝑒,𝑎)=0\nQ_table[2,2,:] = 0\n\nmax_episode = 10000\nmax_step = 100\n\nprint(\"start TD(0) control : Q-learning\")\nalpha = 0.1\nepsilon = 0.8\n\n# 각 에피소드에 대해 반복 :\nfor epi in tqdm(range(max_episode)):\n    dleta = 0\n    # S 를 초기화\n    i = 0\n    j = 0\n    agent.set_pos([i,j])\n\n    # 에피소드의 각 스텝에 대해 반복 :\n    for k in range(max_step):\n        # s 에서 행동 정책(Behavior policy)으로 행동 a를 선택 ( 예 : ε-greedy\n        pos = agent.get_pos()\n        action = e_greedy(Q_table,agent,epsilon)\n        # 행동 a 를 취한 후 보상 r과 다음 상태 s^'를 관측\n        observation, reward, done = env.move(agent, action)\n\n        # s^' 에서 타깃 정책(Target policy)으로 행동 a^'를 선택 ( 예 : greedy)\n\n        next_action = greedy(Q_table,agent,epsilon)\n        \n        # Q(s,a)←Q(s,a) + α[r+𝛾  maxa'𝑄(s',a')−𝑄(s,a)] \n        Q_table[pos[0],pos[1],action] += alpha * (reward + gamma * Q_table[observation[0], observation[1],next_action] - Q_table[pos[0],pos[1],action])\n        # s가 마지막 상태라면 종료\n        if done == True:\n            break\n        \n# 학습된 정책에서 최적 행동 추출\noptimal_policy = np.zeros((env.reward.shape[0], env.reward.shape[1]))\nfor i in range(env.reward.shape[0]):\n    for j in range(env.reward.shape[1]):\n        optimal_policy[i,j] = np.argmax(Q_table[i,j,:])\n\nprint(\"Q-learning : Q(s,a)\")\nshow_q_table(np.round(Q_table,2),env)\nprint(\"Q-learning :optimal policy\")\nshow_policy(optimal_policy,env)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Double Q-learning"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#  Double Q-learning with ε-greedy\nnp.random.seed(0)\n\n# 환경, 에이전트를 초기화\nenv = Environment()\nagent = Agent()\ngamma = 0.9\n\n# 모든 𝑠∈𝑆,𝑎∈𝐴(𝑆)에 대해 초기화:\n# 𝑄1 (𝑠,𝑎),𝑄2 (𝑠,𝑎)←임의의 값\nQ1_table = np.random.rand(env.reward.shape[0], env.reward.shape[1],len(agent.action))\nQ2_table = np.random.rand(env.reward.shape[0], env.reward.shape[1],len(agent.action))\n# 𝑄1 (𝑡𝑒𝑟𝑚𝑖𝑛𝑎𝑙−𝑠𝑡𝑎𝑡𝑒,)=𝑄2 (𝑡𝑒𝑟𝑚𝑖𝑛𝑎𝑙−𝑠𝑡𝑎𝑡𝑒,)=0 \nQ1_table[2,2,:] = 0\nQ2_table[2,2,:] = 0\n\nmax_episode = 10000\nmax_step = 10\n\nprint(\"start Double Q-learning\")\nalpha = 0.1\nepsilon = 0.3\n\n# 각 에피소드에 대해 반복 :\nfor epi in tqdm(range(max_episode)):\n    # S 를 초기화\n    i = 0\n    j = 0\n    agent.set_pos([i,j])\n    \n    # 에피소드의 각 스텝에 대해 반복 :\n    for k in range(max_step):\n        pos = agent.get_pos()\n        # 𝑄1과 𝑄2로 부터 a를 선택 (예 : 𝜖−𝑔𝑟𝑒𝑒𝑑𝑦 in 𝑄1+𝑄2)\n        Q = Q1_table + Q2_table\n        action = e_greedy(Q,agent,epsilon)\n        # 행동 a 를 취한 후 보상 r과 다음 상태 s'를 관측\n        observation, reward, done = env.move(agent, action)\n\n        # S’에서 Target policy 행동 A’를 선택 ( 예 : 𝑔𝑟𝑒𝑒𝑑𝑦)\n        p = np.random.random()\n        # 확률이 0.5보다 작다면\n        if (p<0.5):\n            next_action = greedy(Q1_table,agent,epsilon)\n            # 𝑄1 (𝑠,𝑎)←𝑄1 (𝑠,𝑎)+ α[𝑅+𝛾𝑄2 (𝑆',argmaxaQ1(s',a)−𝑄1 (𝑠,𝑎)]        \n            Q1_table[pos[0],pos[1],action] += alpha * (reward + gamma*Q2_table[observation[0],observation[1],next_action] - Q1_table[pos[0],pos[1],action])\n        else:\n            next_action = greedy(Q2_table,agent,epsilon)\n            # 𝑄2 (𝑠,𝑎)←𝑄2 (𝑠,𝑎)+ α[𝑅+𝛾𝑄1 (𝑆',argmaxQ2(s',a)−−𝑄2 (𝑠,𝑎)] \n            Q2_table[pos[0],pos[1],action] += alpha * (reward + gamma*Q1_table[observation[0],observation[1],next_action] - Q2_table[pos[0],pos[1],action])\n        # s가 마지막 상태라면 종료\n        if done == True:\n            break\n        \n# 학습된 정책에서 최적 행동 추출\noptimal_policy = np.zeros((env.reward.shape[0],env.reward.shape[1]))\nfor i in range(env.reward.shape[0]):\n    for j in range(env.reward.shape[1]):\n        optimal_policy[i,j] = np.argmax(Q1_table[i,j,:]+Q2_table[i,j,:])\nprint(\"Double Q-learning : Q1(s,a)\")\nshow_q_table(np.round(Q1_table,2),env)\nprint(\"Double Q-learning : Q2(s,a)\")\nshow_q_table(np.round(Q2_table,2),env)\nprint(\"Double Q-learning : Q1(s,a)+Q2(s,a)\")\nshow_q_table(np.round(Q1_table+Q2_table,2),env)\nprint(\"Double Q-learning :optimal policy\")\nshow_policy(optimal_policy,env)        \n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Actor-Critic"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#  액터-크리틱알고리즘\nnp.random.seed(0)\n\n# 환경, 에이전트를 초기화\nenv = Environment()\nagent = Agent()\ngamma = 0.9\n\n# p(𝑠,𝑎), 𝑉(𝑠)←임의의 값\nV = np.random.rand(env.reward.shape[0], env.reward.shape[1])\npolicy = np.random.rand(env.reward.shape[0], env.reward.shape[1],len(agent.action))\n# 확률의 합이 1이 되도록 변환\nfor i in range(env.reward.shape[0]):\n    for j in range(env.reward.shape[1]):\n        policy[i,j,:] = policy[i,j,:] /np.sum(policy[i,j,:])\n\nmax_episode = 10000\nmax_step = 100\n\nprint(\"start Actor-Critic\")\nalpha = 0.1\n\n# 각 에피소드에 대해 반복 :\nfor epi in tqdm(range(max_episode)):\n    # S 를 초기화\n    i = 0\n    j = 0\n    agent.set_pos([i,j])\n\n    # 에피소드의 각 스텝에 대해 반복 :\n    for k in range(max_step):\n        \n        # Actor : p(𝑠,𝑎)로 부터 a를 선택 ( 예 :Gibbs softmax method)\n        pos = agent.get_pos()\n        \n        # Gibbs softmax method 로 선택될 확률을 조정\n        pr = np.zeros(4)\n        for i in range(len(agent.action)):\n            pr[i] = np.exp(policy[pos[0],pos[1],i])/np.sum(np.exp(policy[pos[0],pos[1],:]))\n                \n        # 행동 선택\n        action =  np.random.choice(range(0,len(agent.action)), p=pr)            \n        \n        # 행동 a를 취한 후 보상 r과 다음 상태 s'를 관측\n        observation, reward, done = env.move(agent, action)\n        \n        # Critic 학습\n        # δt=r(t+1)+γV(S(t+1) )-V(St)\n        td_error = reward + gamma * V[observation[0],observation[1]] - V[pos[0],pos[1]]\n        V[pos[0],pos[1]] += alpha * td_error\n\n        # Actor 학습 :\n        # p(st,at)=p(st,at)-βδ_t\n        policy[pos[0],pos[1],action] += td_error * 0.01\n        \n        # 확률에 음수가 있을경우 전부 양수가 되도록 보정\n        if np.min(policy[pos[0],pos[1],:]) < 0:\n            policy[pos[0],pos[1],:] -= np.min(policy[pos[0],pos[1],:])\n        for i in range(env.reward.shape[0]):\n            for j in range(env.reward.shape[1]):\n                policy[i,j,:] = policy[i,j,:] /np.sum(policy[i,j,:])\n        \n        # s가 마지막 상태라면 종료\n        if done == True:\n            break\n\n# 학습된 정책에서 최적 행동 추출\noptimal_policy = np.zeros((env.reward.shape[0],env.reward.shape[1]))\nfor i in range(env.reward.shape[0]):\n    for j in range(env.reward.shape[1]):\n        optimal_policy[i,j] = np.argmax(policy[i,j,:])\n        \nprint(\"Actor - Critic : V(s)\")\nshow_v_table(np.round(V,2),env)\nprint(\"Actor - Critic : policy(s,a)\")\nshow_q_table(np.round(policy,2),env)\nprint(\"Actor - Critic : optimal policy\")\nshow_policy(optimal_policy,env)   ",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### TD(0) 함수 근사"
    },
    {
      "metadata": {
        "scrolled": false,
        "trusted": true
      },
      "cell_type": "code",
      "source": "# TD(0) 함수 근사\nnp.random.seed(1)\n\n# 환경, 에이전트를 초기화\nenv = Environment()\nagent = Agent()\ngamma = 0.9\n\n#초기화 : \n# 𝑣(𝑠│𝒘)← 미분 가능한 함수\n# w← 함수의 가중치를 임의의 값으로 초기화\n# w[0]+ w[1] * x1  + w[1] * x2\nw = np.random.rand(env.reward.shape[0])\nw -= 0.5\n\nv_table = np.zeros((env.reward.shape[0], env.reward.shape[1]))\nfor i in range(env.reward.shape[0]):\n    for j in range(env.reward.shape[1]):\n        v_table[i,j] = w[0] + w[1] * i + w[2] * j\n\n        \nprint(\"Before : TD(0) Function Approximation : v(s|w)\")\nprint()\nprint(\"Initial w\")\nprint(\"w = {}\".format(np.round(w,2)))\nshow_v_table(np.round(v_table,2),env)\n\n# 최대 에피소드, 에피소드의 최대 길이를 지정\nmax_episode = 10000\nmax_step = 100\nalpha = 0.01\nepsilon = 0.3 \nprint(\"start Function Approximation TD(0) prediction\")\n\n# 각 에피소드에 대해 반복 :\nfor epi in tqdm(range(max_episode)):\n    delta =0\n    # s 를 초기화\n    i = 0\n    j = 0\n    agent.set_pos([i,j])\n    temp =0\n    #  에피소드의 각 스텝에 대해 반복 :\n    for k in range(max_step):\n        pos = agent.get_pos()\n        # 무작위로 행동 선택\n        action = np.random.randint(0,len(agent.action))   \n        # 행동 a 를 취한 후 보상 r과 다음 상태 s'를 관측\n        observation, reward, done = env.move(agent,action)\n        now_v =0\n        next_v =0\n        # 현재상태가치함수와 다음 상태가치함수를 𝑣(𝑠│𝒘)로부터 계산\n        now_v = w[0] + np.dot(w[1:],pos)\n        next_v = w[0] + np.dot(w[1:],observation)    \n        # w←𝑤+𝛼[𝑟−𝑣(𝑠│𝒘)](𝜕𝑣(s│𝒘))/𝜕𝑤\n        w[0] += alpha * ( reward + gamma * next_v - now_v )\n        w[1] += alpha * ( reward + gamma * next_v - now_v ) * pos[0]\n        w[2] += alpha * ( reward + gamma * next_v - now_v ) * pos[1]\n\n        # s가 마지막 상태라면 종료\n        if done == True:\n            break\n\nfor i in range(env.reward.shape[0]):\n    for j in range(env.reward.shape[1]):\n        v_table[i,j] = w[0] + w[1] * i + w[2] * j\n\nprint()        \nprint(\"After : TD(0) Function Approximation : v(s|w)\")\nprint()\nprint(\"Final w\")\nprint(\"w = {}\".format(np.round(w,2)))\n\nprint(\"TD(0) Function Approximation : V(s)\")\nshow_v_table(np.round(v_table,2),env)\n\nprint()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {},
      "cell_type": "markdown",
      "source": "### Q-learning 함수근사"
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "#  Q-learning 함수근사\nnp.random.seed(0)\n\n# 환경, 에이전트를 초기화\nenv = Environment()\nagent = Agent()\ngamma = 0.9\n\n# 초기화 : \n# 𝑞(𝑠,𝑎│𝒘)← 미분 가능한 함수\n# w← 함수의 가중치를 임의의 값으로 초기화\n\nw = np.random.rand(len(agent.action),env.reward.shape[0])\nw -= 0.5\n\nFA_Q_table = np.zeros((env.reward.shape[0],env.reward.shape[1],len(agent.action)))\n# 함수를 테이블에 저장\nfor i in range(env.reward.shape[0]):\n    for j in range(env.reward.shape[1]):\n        for k in range(len(agent.action)):\n            FA_Q_table[i,j,k] = w[k,0]  + w[k,1] * i + w[k,2]* j          \n\n# 학습된 정책에서 최적 행동 추출\noptimal_policy = np.zeros((env.reward.shape[0], env.reward.shape[1]))\nfor i in range(env.reward.shape[0]):\n    for j in range(env.reward.shape[1]):\n        optimal_policy[i,j] = np.argmax(FA_Q_table[i,j,:])\n\n\nprint(\"Before : Function Approximation Q-learning : Q(s,a|w)\")\nshow_q_table(np.round(FA_Q_table,2),env)\nprint()\nprint(\"Before : Function Approximation Q-learning :optimal policy\")\nshow_policy(optimal_policy,env)  \nprint()\nprint(\"Initial w\")\nprint(\"w = {}\".format(np.round(w,2)))\nprint()\n\n\nmax_episode = 100000\nmax_step = 100\n\nprint(\"start Function Approximation : Q-learning\")\n\nalpha = 0.01\n\n#각 에피소드에 대해 반복 :\nfor epi in tqdm(range(max_episode)):\n    # s 를 초기화\n    i = 0\n    j = 0\n    agent.set_pos([i,j])\n\n    # 에피소드의 각 스텝에 대해 반복 :\n    for k in range(max_step):\n        pos = agent.get_pos()\n        # s에서 Behavior policy로 행동 a를 선택 (Gibbs 소프트맥스 함수사용)\n        action = np.zeros(4)\n        for act in range(len(agent.action)):\n            action[act] = w[act,0]  + w[act,1]* pos[0] + w[act,2]*pos[1]\n            \n        pr = np.zeros(4)\n        for i in range(len(agent.action)):\n            pr[i] = np.exp(action[i])/np.sum(np.exp(action[:]))\n                \n        action = np.random.choice(range(0,len(agent.action)), p=pr)  \n\n        # 행동 a 를 취한 후 보상 r과 다음 상태 s’를 관측\n        observation, reward, done = env.move(agent,action)\n        \n        # s’ 에서 Target policy 행동 a'를 선택 (𝑔𝑟𝑒𝑒𝑑𝑦)\n        next_act = np.zeros(4)\n        for act in range(len(agent.action)):\n            next_act[act] = np.dot(w[act,1:],observation) + w[act,0]\n        best_action = np.argmax(next_act)\n        \n        now_q = np.dot(w[action,1:],pos) + w[action,0]\n        next_q = np.dot(w[best_action,1:],observation) + w[best_action,0]\n       \n        # w 갱신\n        w[action,0] += alpha * (reward + gamma * next_q - now_q)\n        w[action,1] += alpha * (reward + gamma * next_q - now_q) * pos[0]\n        w[action,2] += alpha * (reward + gamma * next_q - now_q) * pos[1]\n        \n        # s가 마지막 상태라면 종료\n        if done == True:\n            break\n            \n# FA_Q = np.zeros((len(agent.action),env.reward.shape[0],env.reward.shape[1]))\n# 함수를 테이블에 저장\nfor i in range(env.reward.shape[0]):\n    for j in range(env.reward.shape[1]):\n        for k in range(len(agent.action)):\n            FA_Q_table[i,j,k] = w[k,0]  + w[k,1] * i + w[k,2]* j          \n\n# 학습된 정책에서 최적 행동 추출\n# optimal_policy = np.zeros((env.reward.shape[0], env.reward.shape[1]))\nfor i in range(env.reward.shape[0]):\n    for j in range(env.reward.shape[1]):\n        optimal_policy[i,j] = np.argmax(FA_Q_table[i,j,:])\n\n    \nprint(\"After : Function Approximation Q-learning : Q(s,a|w)\")\nshow_q_table(np.round(FA_Q_table,2),env)\nprint()\nprint(\"After : Function Approximation Q-learning :optimal policy\")\nshow_policy(optimal_policy,env)  \nprint()\nprint(\"Final w\")\nprint(\"w = {}\".format(np.round(w,2)))\nprint()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}